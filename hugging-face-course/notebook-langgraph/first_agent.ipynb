{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d5d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import mimetypes\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from typing import Optional\n",
    "\n",
    "from smolagents.agent_types import AgentAudio, AgentImage, AgentText, handle_agent_output_types\n",
    "from smolagents.agents import ActionStep, MultiStepAgent\n",
    "from smolagents.memory import MemoryStep\n",
    "from smolagents.utils import _is_package_available\n",
    "\n",
    "\n",
    "def pull_messages_from_step(\n",
    "    step_log: MemoryStep,\n",
    "):\n",
    "    \"\"\"Extract ChatMessage objects from agent steps with proper nesting\"\"\"\n",
    "    import gradio as gr\n",
    "\n",
    "    if isinstance(step_log, ActionStep):\n",
    "        # Output the step number\n",
    "        step_number = f\"Step {step_log.step_number}\" if step_log.step_number is not None else \"\"\n",
    "        yield gr.ChatMessage(role=\"assistant\", content=f\"**{step_number}**\")\n",
    "\n",
    "        # First yield the thought/reasoning from the LLM\n",
    "        if hasattr(step_log, \"model_output\") and step_log.model_output is not None:\n",
    "            # Clean up the LLM output\n",
    "            model_output = step_log.model_output.strip()\n",
    "            # Remove any trailing <end_code> and extra backticks, handling multiple possible formats\n",
    "            model_output = re.sub(r\"```\\s*<end_code>\", \"```\", model_output)  # handles ```<end_code>\n",
    "            model_output = re.sub(r\"<end_code>\\s*```\", \"```\", model_output)  # handles <end_code>```\n",
    "            model_output = re.sub(r\"```\\s*\\n\\s*<end_code>\", \"```\", model_output)  # handles ```\\n<end_code>\n",
    "            model_output = model_output.strip()\n",
    "            yield gr.ChatMessage(role=\"assistant\", content=model_output)\n",
    "\n",
    "        # For tool calls, create a parent message\n",
    "        if hasattr(step_log, \"tool_calls\") and step_log.tool_calls is not None:\n",
    "            first_tool_call = step_log.tool_calls[0]\n",
    "            used_code = first_tool_call.name == \"python_interpreter\"\n",
    "            parent_id = f\"call_{len(step_log.tool_calls)}\"\n",
    "\n",
    "            # Tool call becomes the parent message with timing info\n",
    "            # First we will handle arguments based on type\n",
    "            args = first_tool_call.arguments\n",
    "            if isinstance(args, dict):\n",
    "                content = str(args.get(\"answer\", str(args)))\n",
    "            else:\n",
    "                content = str(args).strip()\n",
    "\n",
    "            if used_code:\n",
    "                # Clean up the content by removing any end code tags\n",
    "                content = re.sub(r\"```.*?\\n\", \"\", content)  # Remove existing code blocks\n",
    "                content = re.sub(r\"\\s*<end_code>\\s*\", \"\", content)  # Remove end_code tags\n",
    "                content = content.strip()\n",
    "                if not content.startswith(\"```python\"):\n",
    "                    content = f\"```python\\n{content}\\n```\"\n",
    "\n",
    "            parent_message_tool = gr.ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=content,\n",
    "                metadata={\n",
    "                    \"title\": f\"üõ†Ô∏è Used tool {first_tool_call.name}\",\n",
    "                    \"id\": parent_id,\n",
    "                    \"status\": \"pending\",\n",
    "                },\n",
    "            )\n",
    "            yield parent_message_tool\n",
    "\n",
    "            # Nesting execution logs under the tool call if they exist\n",
    "            if hasattr(step_log, \"observations\") and (\n",
    "                step_log.observations is not None and step_log.observations.strip()\n",
    "            ):  # Only yield execution logs if there's actual content\n",
    "                log_content = step_log.observations.strip()\n",
    "                if log_content:\n",
    "                    log_content = re.sub(r\"^Execution logs:\\s*\", \"\", log_content)\n",
    "                    yield gr.ChatMessage(\n",
    "                        role=\"assistant\",\n",
    "                        content=f\"{log_content}\",\n",
    "                        metadata={\"title\": \"üìù Execution Logs\", \"parent_id\": parent_id, \"status\": \"done\"},\n",
    "                    )\n",
    "\n",
    "            # Nesting any errors under the tool call\n",
    "            if hasattr(step_log, \"error\") and step_log.error is not None:\n",
    "                yield gr.ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=str(step_log.error),\n",
    "                    metadata={\"title\": \"üí• Error\", \"parent_id\": parent_id, \"status\": \"done\"},\n",
    "                )\n",
    "\n",
    "            # Update parent message metadata to done status without yielding a new message\n",
    "            parent_message_tool.metadata[\"status\"] = \"done\"\n",
    "\n",
    "        # Handle standalone errors but not from tool calls\n",
    "        elif hasattr(step_log, \"error\") and step_log.error is not None:\n",
    "            yield gr.ChatMessage(role=\"assistant\", content=str(step_log.error), metadata={\"title\": \"üí• Error\"})\n",
    "\n",
    "        # Calculate duration and token information\n",
    "        step_footnote = f\"{step_number}\"\n",
    "        if hasattr(step_log, \"input_token_count\") and hasattr(step_log, \"output_token_count\"):\n",
    "            token_str = (\n",
    "                f\" | Input-tokens:{step_log.input_token_count:,} | Output-tokens:{step_log.output_token_count:,}\"\n",
    "            )\n",
    "            step_footnote += token_str\n",
    "        if hasattr(step_log, \"duration\"):\n",
    "            step_duration = f\" | Duration: {round(float(step_log.duration), 2)}\" if step_log.duration else None\n",
    "            step_footnote += step_duration\n",
    "        step_footnote = f\"\"\"<span style=\"color: #bbbbc2; font-size: 12px;\">{step_footnote}</span> \"\"\"\n",
    "        yield gr.ChatMessage(role=\"assistant\", content=f\"{step_footnote}\")\n",
    "        yield gr.ChatMessage(role=\"assistant\", content=\"-----\")\n",
    "\n",
    "\n",
    "def stream_to_gradio(\n",
    "    agent,\n",
    "    task: str,\n",
    "    reset_agent_memory: bool = False,\n",
    "    additional_args: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages.\"\"\"\n",
    "    if not _is_package_available(\"gradio\"):\n",
    "        raise ModuleNotFoundError(\n",
    "            \"Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`\"\n",
    "        )\n",
    "    import gradio as gr\n",
    "\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    for step_log in agent.run(task, stream=True, reset=reset_agent_memory, additional_args=additional_args):\n",
    "        # Track tokens if model provides them\n",
    "        if hasattr(agent.model, \"last_input_token_count\"):\n",
    "            total_input_tokens += agent.model.last_input_token_count\n",
    "            total_output_tokens += agent.model.last_output_token_count\n",
    "            if isinstance(step_log, ActionStep):\n",
    "                step_log.input_token_count = agent.model.last_input_token_count\n",
    "                step_log.output_token_count = agent.model.last_output_token_count\n",
    "\n",
    "        for message in pull_messages_from_step(\n",
    "            step_log,\n",
    "        ):\n",
    "            yield message\n",
    "\n",
    "    final_answer = step_log  # Last log is the run's final_answer\n",
    "    final_answer = handle_agent_output_types(final_answer)\n",
    "\n",
    "    if isinstance(final_answer, AgentText):\n",
    "        yield gr.ChatMessage(\n",
    "            role=\"assistant\",\n",
    "            content=f\"**Final answer:**\\n{final_answer.to_string()}\\n\",\n",
    "        )\n",
    "    elif isinstance(final_answer, AgentImage):\n",
    "        yield gr.ChatMessage(\n",
    "            role=\"assistant\",\n",
    "            content={\"path\": final_answer.to_string(), \"mime_type\": \"image/png\"},\n",
    "        )\n",
    "    elif isinstance(final_answer, AgentAudio):\n",
    "        yield gr.ChatMessage(\n",
    "            role=\"assistant\",\n",
    "            content={\"path\": final_answer.to_string(), \"mime_type\": \"audio/wav\"},\n",
    "        )\n",
    "    else:\n",
    "        yield gr.ChatMessage(role=\"assistant\", content=f\"**Final answer:** {str(final_answer)}\")\n",
    "\n",
    "\n",
    "class GradioUI:\n",
    "    \"\"\"A one-line interface to launch your agent in Gradio\"\"\"\n",
    "\n",
    "    def __init__(self, agent: MultiStepAgent, file_upload_folder: str | None = None):\n",
    "        if not _is_package_available(\"gradio\"):\n",
    "            raise ModuleNotFoundError(\n",
    "                \"Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`\"\n",
    "            )\n",
    "        self.agent = agent\n",
    "        self.file_upload_folder = file_upload_folder\n",
    "        if self.file_upload_folder is not None:\n",
    "            if not os.path.exists(file_upload_folder):\n",
    "                os.mkdir(file_upload_folder)\n",
    "\n",
    "    def interact_with_agent(self, prompt, messages):\n",
    "        import gradio as gr\n",
    "\n",
    "        messages.append(gr.ChatMessage(role=\"user\", content=prompt))\n",
    "        yield messages\n",
    "        for msg in stream_to_gradio(self.agent, task=prompt, reset_agent_memory=False):\n",
    "            messages.append(msg)\n",
    "            yield messages\n",
    "        yield messages\n",
    "\n",
    "    def upload_file(\n",
    "        self,\n",
    "        file,\n",
    "        file_uploads_log,\n",
    "        allowed_file_types=[\n",
    "            \"application/pdf\",\n",
    "            \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "            \"text/plain\",\n",
    "        ],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle file uploads, default allowed types are .pdf, .docx, and .txt\n",
    "        \"\"\"\n",
    "        import gradio as gr\n",
    "\n",
    "        if file is None:\n",
    "            return gr.Textbox(\"No file uploaded\", visible=True), file_uploads_log\n",
    "\n",
    "        try:\n",
    "            mime_type, _ = mimetypes.guess_type(file.name)\n",
    "        except Exception as e:\n",
    "            return gr.Textbox(f\"Error: {e}\", visible=True), file_uploads_log\n",
    "\n",
    "        if mime_type not in allowed_file_types:\n",
    "            return gr.Textbox(\"File type disallowed\", visible=True), file_uploads_log\n",
    "\n",
    "        # Sanitize file name\n",
    "        original_name = os.path.basename(file.name)\n",
    "        sanitized_name = re.sub(\n",
    "            r\"[^\\w\\-.]\", \"_\", original_name\n",
    "        )  # Replace any non-alphanumeric, non-dash, or non-dot characters with underscores\n",
    "\n",
    "        type_to_ext = {}\n",
    "        for ext, t in mimetypes.types_map.items():\n",
    "            if t not in type_to_ext:\n",
    "                type_to_ext[t] = ext\n",
    "\n",
    "        # Ensure the extension correlates to the mime type\n",
    "        sanitized_name = sanitized_name.split(\".\")[:-1]\n",
    "        sanitized_name.append(\"\" + type_to_ext[mime_type])\n",
    "        sanitized_name = \"\".join(sanitized_name)\n",
    "\n",
    "        # Save the uploaded file to the specified folder\n",
    "        file_path = os.path.join(self.file_upload_folder, os.path.basename(sanitized_name))\n",
    "        shutil.copy(file.name, file_path)\n",
    "\n",
    "        return gr.Textbox(f\"File uploaded: {file_path}\", visible=True), file_uploads_log + [file_path]\n",
    "\n",
    "    def log_user_message(self, text_input, file_uploads_log):\n",
    "        return (\n",
    "            text_input\n",
    "            + (\n",
    "                f\"\\nYou have been provided with these files, which might be helpful or not: {file_uploads_log}\"\n",
    "                if len(file_uploads_log) > 0\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"\",\n",
    "        )\n",
    "\n",
    "    def launch(self, **kwargs):\n",
    "        import gradio as gr\n",
    "\n",
    "        with gr.Blocks(fill_height=True) as demo:\n",
    "            stored_messages = gr.State([])\n",
    "            file_uploads_log = gr.State([])\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Agent\",\n",
    "                type=\"messages\",\n",
    "                avatar_images=(\n",
    "                    None,\n",
    "                    \"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/Alfred.png\",\n",
    "                ),\n",
    "                resizeable=True,\n",
    "                scale=1,\n",
    "            )\n",
    "            # If an upload folder is provided, enable the upload feature\n",
    "            if self.file_upload_folder is not None:\n",
    "                upload_file = gr.File(label=\"Upload a file\")\n",
    "                upload_status = gr.Textbox(label=\"Upload Status\", interactive=False, visible=False)\n",
    "                upload_file.change(\n",
    "                    self.upload_file,\n",
    "                    [upload_file, file_uploads_log],\n",
    "                    [upload_status, file_uploads_log],\n",
    "                )\n",
    "            text_input = gr.Textbox(lines=1, label=\"Chat Message\")\n",
    "            text_input.submit(\n",
    "                self.log_user_message,\n",
    "                [text_input, file_uploads_log],\n",
    "                [stored_messages, text_input],\n",
    "            ).then(self.interact_with_agent, [stored_messages, chatbot], [chatbot])\n",
    "\n",
    "        demo.launch(debug=True, share=True, **kwargs)\n",
    "\n",
    "\n",
    "__all__ = [\"stream_to_gradio\", \"GradioUI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e20d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smolagents[gradio] in /opt/anaconda3/lib/python3.12/site-packages (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (0.34.4)\n",
      "Requirement already satisfied: requests>=2.32.3 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (2.32.3)\n",
      "Requirement already satisfied: rich>=13.9.4 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (14.1.0)\n",
      "Requirement already satisfied: jinja2>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (3.1.4)\n",
      "Requirement already satisfied: pillow<11.2.0,>=11.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (11.1.0)\n",
      "Requirement already satisfied: markdownify>=0.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (1.2.0)\n",
      "Requirement already satisfied: duckduckgo-search>=6.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (8.1.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (1.1.1)\n",
      "Requirement already satisfied: gradio>=5.13.2 in /opt/anaconda3/lib/python3.12/site-packages (from smolagents[gradio]) (5.42.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from duckduckgo-search>=6.3.7->smolagents[gradio]) (8.2.1)\n",
      "Requirement already satisfied: primp>=0.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from duckduckgo-search>=6.3.7->smolagents[gradio]) (0.15.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from duckduckgo-search>=6.3.7->smolagents[gradio]) (6.0.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (4.2.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (1.11.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.27.0)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (3.10.15)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (2.2.2)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (2.8.2)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.12.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (4.11.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio>=5.13.2->smolagents[gradio]) (0.34.1)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.11.1->gradio>=5.13.2->smolagents[gradio]) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.11.1->gradio>=5.13.2->smolagents[gradio]) (15.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.0->smolagents[gradio]) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.0->smolagents[gradio]) (4.66.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.28.0->smolagents[gradio]) (1.1.7)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from markdownify>=0.14.1->smolagents[gradio]) (4.12.3)\n",
      "Requirement already satisfied: six<2,>=1.15 in /opt/anaconda3/lib/python3.12/site-packages (from markdownify>=0.14.1->smolagents[gradio]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.3->smolagents[gradio]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.3->smolagents[gradio]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.3->smolagents[gradio]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.3->smolagents[gradio]) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=13.9.4->smolagents[gradio]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=13.9.4->smolagents[gradio]) (2.15.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio>=5.13.2->smolagents[gradio]) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.9->markdownify>=0.14.1->smolagents[gradio]) (2.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio>=5.13.2->smolagents[gradio]) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio>=5.13.2->smolagents[gradio]) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents[gradio]) (0.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio>=5.13.2->smolagents[gradio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio>=5.13.2->smolagents[gradio]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio>=5.13.2->smolagents[gradio]) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio>=5.13.2->smolagents[gradio]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio>=5.13.2->smolagents[gradio]) (2.20.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=5.13.2->smolagents[gradio]) (1.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'smolagents[gradio]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a3680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:64: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:72: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:72: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/8b/8ktrkt992xx9kcyjf8n9b2340000gn/T/ipykernel_63698/3562095868.py:64: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  return '''Here's an ASCII cat for you:\n",
      "/var/folders/8b/8ktrkt992xx9kcyjf8n9b2340000gn/T/ipykernel_63698/3562095868.py:72: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  return '''Here's an ASCII dog for you:\n",
      "/var/folders/8b/8ktrkt992xx9kcyjf8n9b2340000gn/T/ipykernel_63698/3562095868.py:64: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  return '''Here's an ASCII cat for you:\n",
      "/var/folders/8b/8ktrkt992xx9kcyjf8n9b2340000gn/T/ipykernel_63698/3562095868.py:72: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  return '''Here's an ASCII dog for you:\n"
     ]
    },
    {
     "ename": "DocstringParsingException",
     "evalue": "Cannot generate JSON schema for generate_simple_image because the docstring has no description for the argument 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDocstringParsingException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m     prompt_templates = yaml.safe_load(stream)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Create a simple image generation tool\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;129m@tool\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mgenerate_simple_image\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    Generate a simple ASCII art image based on the prompt.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    For real image generation, you'd need to integrate with image APIs.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m prompt.lower():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/tools.py:901\u001b[39m, in \u001b[36mtool\u001b[39m\u001b[34m(tool_function)\u001b[39m\n\u001b[32m    891\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mtool\u001b[39m(tool_function: Callable) -> Tool:\n\u001b[32m    892\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[33;03m    Convert a function into an instance of a dynamically created Tool subclass.\u001b[39;00m\n\u001b[32m    894\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    899\u001b[39m \u001b[33;03m            and an 'Args:' part where each argument is described.\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     tool_json_schema = get_json_schema(tool_function)[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tool_json_schema:\n\u001b[32m    903\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TypeHintParsingException(\u001b[33m\"\u001b[39m\u001b[33mTool return type not found: make sure your function has a return type hint!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/_function_type_hints_utils.py:203\u001b[39m, in \u001b[36mget_json_schema\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg, schema \u001b[38;5;129;01min\u001b[39;00m json_schema[\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m].items():\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_descriptions:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m DocstringParsingException(\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot generate JSON schema for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because the docstring has no description for the argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         )\n\u001b[32m    206\u001b[39m     desc = param_descriptions[arg]\n\u001b[32m    207\u001b[39m     enum_choices = re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m(choices:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(.*?)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*$\u001b[39m\u001b[33m\"\u001b[39m, desc, flags=re.IGNORECASE)\n",
      "\u001b[31mDocstringParsingException\u001b[39m: Cannot generate JSON schema for generate_simple_image because the docstring has no description for the argument 'prompt'"
     ]
    }
   ],
   "source": [
    "# First, let's install the required gradio package\n",
    "from smolagents import CodeAgent,DuckDuckGoSearchTool, HfApiModel,load_tool,tool\n",
    "import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import yaml\n",
    "from tools.final_answer import FinalAnswerTool\n",
    "from smolagents import LiteLLMModel\n",
    "\n",
    "from Gradio_UI import GradioUI\n",
    "\n",
    "# Below is an example of a tool that does nothing. Amaze us with your creativity !\n",
    "@tool\n",
    "def my_custom_tool(arg1:str, arg2:int)-> str: #it's import to specify the return type\n",
    "    #Keep this format for the description / args / args description but feel free to modify the tool\n",
    "    \"\"\"A tool that does nothing yet \n",
    "    Args:\n",
    "        arg1: the first argument\n",
    "        arg2: the second argument\n",
    "    \"\"\"\n",
    "    return \"What magic will you build ?\"\n",
    "\n",
    "@tool\n",
    "def get_current_time_in_timezone(timezone: str) -> str:\n",
    "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
    "    Args:\n",
    "        timezone: A string representing a valid timezone (e.g., 'America/New_York').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create timezone object\n",
    "        tz = pytz.timezone(timezone)\n",
    "        # Get current time in that timezone\n",
    "        local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return f\"The current local time in {timezone} is: {local_time}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching time for timezone '{timezone}': {str(e)}\"\n",
    "\n",
    "\n",
    "final_answer = FinalAnswerTool()\n",
    "\n",
    "# If the agent does not answer, the model is overloaded, please use another model or the following Hugging Face Endpoint that also contains qwen2.5 coder:\n",
    "# model_id='https://pflgm2locj2t89co.us-east-1.aws.endpoints.huggingface.cloud' \n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"ollama_chat/qwen2:7b\",  # Or try other Ollama-supported models\n",
    "    api_base=\"http://127.0.0.1:11434\",  # Default Ollama local server\n",
    "    num_ctx=8192,\n",
    ")\n",
    "\n",
    "# Import tool from Hub\n",
    "image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True)\n",
    "\n",
    "with open(\"prompts.yaml\", 'r') as stream:\n",
    "    prompt_templates = yaml.safe_load(stream)\n",
    "    \n",
    "# Create a simple image generation tool\n",
    "@tool\n",
    "def generate_simple_image(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a simple ASCII art image based on the prompt.\n",
    "    For real image generation, you'd need to integrate with image APIs.\n",
    "    \"\"\"\n",
    "    if 'cat' in prompt.lower():\n",
    "        return '''Here's an ASCII cat for you:\n",
    "    /\\_   _/\\\n",
    "   (  o.o  )\n",
    "    ) _ _ (\n",
    "   (  (_)  )\n",
    "    ^^   ^^\n",
    "ASCII art cat generated! For real images, integrate with DALL-E, Stable Diffusion, or similar APIs.'''\n",
    "    elif 'dog' in prompt.lower():\n",
    "        return '''Here's an ASCII dog for you:\n",
    "      / \\   / \\\n",
    "     (   o_o   )\n",
    "      \\  <_>  /\n",
    "       )     (\n",
    "      /       \\\n",
    "     (  (o) (o)  )\n",
    "ASCII art dog generated! For real images, integrate with DALL-E, Stable Diffusion, or similar APIs.'''\n",
    "    else:\n",
    "        return f'''ASCII art for \"{prompt}\":\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ   *  o  *   ‚îÇ\n",
    "    ‚îÇ  o  ‚îå‚îÄ‚îê  o  ‚îÇ\n",
    "    ‚îÇ  *  ‚îÇ ‚îÇ  *  ‚îÇ\n",
    "    ‚îÇ     ‚îî‚îÄ‚îò     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "Simple ASCII representation generated! For real image generation, you would need to:\n",
    "1. Use DALL-E API: openai.Image.create()\n",
    "2. Use Stable Diffusion models\n",
    "3. Use Midjourney API\n",
    "4. Or other image generation services'''\n",
    "\n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[final_answer, generate_simple_image], ## add your tools here (don't remove final answer)\n",
    "    max_steps=6,\n",
    "    verbosity_level=1,\n",
    "    grammar=None,\n",
    "    planning_interval=None,\n",
    "    name=None,\n",
    "    description=None,\n",
    "    prompt_templates=prompt_templates,\n",
    "    # Add authorized imports for image processing\n",
    "    additional_authorized_imports=['PIL', 'numpy', 'matplotlib', 'requests', 'base64', 'io']\n",
    ")\n",
    "\n",
    "\n",
    "GradioUI(agent).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3816802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73a5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
